# Compression_NeuralNets
The Deep Neural Networks have achieved great success in image recognition, speech recognition, and  Natural Language processing tasks. Optimizing more accuracy results made DNN very large with parameters in the order of millions. So large deep neural nets are computationally expensive and memory intensive. So embedding these neural nets into low computation devices made them hard. This project describes the compression models like pruning, quantisation, and knowledge distillation for compressing large neural nets to small with less loss of compression accuracy when compared to the baseline accuracy of network models. 
        
